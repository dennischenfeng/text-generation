{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a48d82-e1f2-48df-b2c9-c8e7cf644fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import Callable, List, Tuple, Iterable, Dict, Type, Any\n",
    "from functools import reduce, lru_cache\n",
    "from collections import OrderedDict\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence\n",
    "# from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "# import optuna\n",
    "# from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "# from optuna.importance import get_param_importances\n",
    "\n",
    "import wandb\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    # GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d092c-48d2-4ae0-89c0-34f52456fae7",
   "metadata": {},
   "source": [
    "# one-time: get hf model from pt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8ba2bc-f6db-4ef4-91a9-73ab25d7205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCausalLMModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hf_model_name: str, \n",
    "        total_steps: int,\n",
    "        lr: float = 5e-5, \n",
    "        weight_decay: float = 0.01,\n",
    "        adam_epsilon: float = 1e-6,\n",
    "        warmup_steps: int = 1000,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # choose this if want blank slate\n",
    "        # self.config = AutoConfig.from_pretrained(\n",
    "        #     \"gpt2\",\n",
    "        #     vocab_size=len(tokenizer),\n",
    "        #     n_ctx=context_length,\n",
    "        #     bos_token_id=tokenizer.bos_token_id,\n",
    "        #     eos_token_id=tokenizer.eos_token_id,\n",
    "        # )\n",
    "        # self.hf_model = GPT2LMHeadModel(self.config)\n",
    "        # self.hf_model(**self.hf_model.dummy_inputs)  # Builds the model\n",
    "\n",
    "        # choose this if want pre-trained weights\n",
    "        self.hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.wandb_table = wandb.Table(columns=[\"step\", \"text\"])\n",
    "        # self.logger.log_table({\"generated_text\": self.wandb_table})\n",
    "    \n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.hf_model(**inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # visualize the output\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\", model=self.hf_model, tokenizer=tokenizer, device=0\n",
    "        )\n",
    "        txt = \"We develop a method to\"\n",
    "        gen_text = pipe(txt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        # self.wandb_table.add_data(self.global_step, gen_text)\n",
    "        # wandb.log({\"generated_text\": self.wandb_table})\n",
    "        # self.logger.log_table({\"generated_text\": self.wandb_table})\n",
    "        print(gen_text)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model = self.hf_model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.hf_model.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.hf_model.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr=self.hparams.lr, \n",
    "            eps=self.hparams.adam_epsilon\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.hparams.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f105ac6c-bcc2-401e-99f7-859cbb897839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitCausalLMModel(\"distilgpt2\", total_steps=1, lr=1e-4)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "76502e12-15ca-4e9f-b052-2372f1567308",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load(\"models/model.ckpt\", map_location=th.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e291730e-7643-4ee5-a6fa-b6709e096280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6ef18-af41-4c9c-94d9-29d21bc6d02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f1d8b53f-fd18-4627-9b6f-3ebd7deeceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7e5b255a-1b16-4fe9-9c95-58b26c9889b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9396ed04-acbd-4da3-99c3-6e01204296c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pt_model.hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "484e6d9e-9611-41f5-9633-0befb31f729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0821f5f5-e727-4334-803a-daef22e17f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/tokenizer_config.json',\n",
       " './models/special_tokens_map.json',\n",
       " './models/vocab.json',\n",
       " './models/merges.txt',\n",
       " './models/added_tokens.json',\n",
       " './models/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "274d688f-9e67-4e92-b340-4869155f1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90ee8e-38a8-47b6-9531-2067fcf9a4b5",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61baa9af-4424-4134-87af-3dc9ac964559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf09bbb8-50d5-40d5-8d2f-dc646fbe2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./models/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6745e055-270f-4ff6-b73e-c8c9aea4aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7c4a76-0dc1-4cc9-8021-24ecbca07f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = (\n",
    "#     \"Increasingly many\"\n",
    "# )\n",
    "# gen_text = pipe(\n",
    "#     txt, \n",
    "#     num_return_sequences=1, \n",
    "#     temperature=1.0,\n",
    "#     top_p=1.0,\n",
    "# )[0][\"generated_text\"]\n",
    "# gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf0ab2a-eb02-436e-95db-66d6b1534f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(gen_text, return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e06446-c839-49a5-8f44-dd9a88284fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6eb7db-85d5-45c8-be8c-4d78017b718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.task_specific_params[\"text-generation\"][\"max_length\"] = 100\n",
    "# pipe = pipeline(\"text-generation\", model=model.hf_model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9841e557-fe1a-4afd-a236-2c1c87093427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function: take last k tokens (or less) from user input, \n",
    "# apply model, then output last N-k tokens\n",
    "\n",
    "# init_text = \"Using a CNN, we propose a method to\"\n",
    "# max_input_tokens = 3\n",
    "# tokenizer\n",
    "# pipeline\n",
    "# temperature = 1.0\n",
    "# top_p = 1.0\n",
    "\n",
    "def generate_new_text(\n",
    "    init_text: str, \n",
    "    pipeline: Callable, \n",
    "    max_last_input_tokens: int, \n",
    "    temperature=1.0, \n",
    "    top_p=1.0,\n",
    "):\n",
    "    tokenizer = pipeline.tokenizer\n",
    "\n",
    "    init_text_tk = tokenizer(init_text, return_length=True)\n",
    "    num_input_tk = min(init_text_tk[\"length\"][0], max_last_input_tokens)\n",
    "\n",
    "    input_text = tokenizer.decode(init_text_tk[\"input_ids\"][-num_input_tk:])\n",
    "    pipeline_output = pipe(input_text, temperature=temperature, top_p=top_p)\n",
    "    output_text = pipeline_output[0][\"generated_text\"]\n",
    "    \n",
    "    output_text_tk = tokenizer(output_text, return_length=True)\n",
    "    new_text = tokenizer.decode(output_text_tk[\"input_ids\"][num_input_tk:])\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130b0853-d9b1-4ee9-b4fb-3d43d242965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_new_text(\n",
    "#     \"Using a CNN, we propose a method to newly\",\n",
    "#     pipe,\n",
    "#     max_last_input_tokens=5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfe502f-20c6-40e7-9c64-f4dd3943b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' the degree of change in data by predicting two different types of transformations and how the transform affects the change in the data. The transformation can be categorized into four categories to identify the transformation and its magnitude. We have also established theoretical guarantees for a wide range of transformations, including transformations that increase the efficiency of the model. We further empirically evaluate the effects of such transformations on the performance of various deep learning architectures. As an example, we find that'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.task_specific_params[\"text-generation\"][\"max_length\"] = 100\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generate_new_text(\n",
    "    \"Using a CNN, we propose a method to newly characterize\",\n",
    "    pipe,\n",
    "    max_last_input_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121d3f6-5908-4ea3-9b73-6145951b285e",
   "metadata": {},
   "source": [
    "# package to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eabd502-bb2c-4bca-beeb-9aff437afbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "from transformers.models.gpt2 import GPT2OnnxConfig\n",
    "from pathlib import Path\n",
    "from transformers.onnx import export, validate_model_outputs\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b89c668-0f8e-469d-b997-3bfc340bcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2 import GPT2OnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4100f79-2364-49f3-bf28-544c9cf0a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config = GPT2OnnxConfig(model.config, task=\"causal-lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d197bd8-8add-4488-9e54-a4a48a7f3065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_config.default_onnx_opset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440e22e0-9624-4532-a0f7-2f4d635e3c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('logits', {0: 'batch', 1: 'sequence'})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_config.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34debdf8-0cb7-4ee3-bac9-dae5be72af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path(\"onnx/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb9b34e0-44c5-4f49-a79f-d03dafd8d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/repos/text-generation/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/Users/dennis/repos/text-generation/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/Users/dennis/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:797: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "/Users/dennis/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    }
   ],
   "source": [
    "onnx_inputs, onnx_outputs = export(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    onnx_config,\n",
    "    onnx_config.default_onnx_opset,\n",
    "    onnx_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c4b6d6-8aea-4a68-a29d-3e1bf43597cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ad7a62d-b4c1-4bf5-a783-3a68b1819473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b454b-86b3-4910-97c1-4d163b86a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c8fff57-5bed-4217-acea-bdc7d6f6e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"onnx/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab5eaa3-ec11-4f8e-a4ae-914308b2e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "104a335d-e9c3-458f-94a2-cbb3390350df",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model_outputs(\n",
    "    onnx_config, tokenizer, model, onnx_path, onnx_outputs, onnx_config.atol_for_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6953bf7-803a-437a-a992-b54b145a46fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a2a60-a55d-44d8-baa9-9e1d841ce211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b58f5a-caa4-4641-8cd1-3ffed5a7f46e",
   "metadata": {},
   "source": [
    "# package to onnx (quantized!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29420a69-053e-4c75-9a1a-11a5b2fb6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "from transformers.models.gpt2 import GPT2OnnxConfig\n",
    "from pathlib import Path\n",
    "from transformers.onnx import export, validate_model_outputs\n",
    "from onnxruntime.quantization import QuantizationMode, quantize\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2abaf590-2026-45c0-b7ce-a7a5f135feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(Path(\"onnx/model.onnx\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d82d9e0-f60c-45a3-bcb1-9e2ca34be556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize(\n",
    "        model=onnx_model,\n",
    "        quantization_mode=QuantizationMode.IntegerOps,\n",
    "        force_fusions=True,\n",
    "        symmetric_weight=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5ce9b9b-d301-41a2-a065-fc697745ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save_model(quantized_model, Path(\"onnx/model_quantized.onnx\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf412fbe-0518-4c30-938f-d9d06d800b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae082a-a9f8-4ece-9a6f-32310349d8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ddcdb79-5296-4c92-b4d4-e4200469c6ff",
   "metadata": {},
   "source": [
    "# Test the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69cd0da5-2d94-4c00-9656-6ffa5dd1f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8756e5f-ae84-42ce-94aa-a8ebcb48173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = InferenceSession(\"onnx/model_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c44d8f54-bc1f-4e23-9fbc-053fd5bf3129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 ms, sys: 1.36 ms, total: 3.15 ms\n",
      "Wall time: 1.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = tokenizer(\"hello hello my name is\", return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd98160-48b9-4e50-876b-586d45749747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[31373, 23748,   616,  1438,   318]]), 'attention_mask': array([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b22e35-e271-426d-9ffc-3ab4f58d5acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.4 ms, sys: 2.7 ms, total: 73.1 ms\n",
      "Wall time: 13.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "outputs = session.run(output_names=['logits'], input_feed=dict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79a3504-6008-4d99-bf00-f86b714f038b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-32.130417, -31.926243, -35.131798, ..., -40.72961 ,\n",
       "          -41.40339 , -35.54015 ],\n",
       "         [-13.938381, -16.497381, -19.185692, ..., -26.25697 ,\n",
       "          -25.579788, -18.103561],\n",
       "         [-12.386647, -15.976733, -17.351515, ..., -22.23131 ,\n",
       "          -23.252188, -16.279594],\n",
       "         [ -9.177689, -12.604435, -15.799781, ..., -22.282354,\n",
       "          -22.306173, -11.495081],\n",
       "         [-10.045435, -10.654558, -13.455166, ..., -20.121496,\n",
       "          -18.981516, -10.902972]]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a8d5393-a9b0-496f-890c-bfe350785c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,   13, 1438,   11,  616]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(outputs[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea4615f-ebf7-46ce-ad6b-461c5370b874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'Ġname', ',', 'Ġmy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(np.argmax(outputs[0], axis=-1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cefa55f1-9959-453a-bf46-35b00bdf2b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',. name, my'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.argmax(outputs[0], axis=-1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe6a0a-6f95-4b84-8980-449f35b8b594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb974df-d25d-4fed-aef5-14ba57b01e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23144c18-6d17-48d4-9d93-ad824a822bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ce0ec1-88f6-47ef-8460-95989a4c8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf model\n",
    "output_pt = model(th.tensor(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4837f5-2c54-41e5-85f9-97396ea6f691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50257])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pt.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1edf290b-5c87-4562-8f0a-4adf0703f9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11,   13, 1438,   11,  616])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_pt.logits.detach().numpy(), axis=-1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bff82e-82ee-4814-a24f-e69d3ce8da06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',. name, my'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.argmax(output_pt.logits.detach().numpy(), axis=-1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575666a0-0734-4fd5-b394-09f02e9eea65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4152d5-003c-4f31-852a-0fab6f09f974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829379a-e947-460e-96e7-2bb0c3aeea54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb966bb9-2e6f-4fa1-b215-95ab16e30ef0",
   "metadata": {},
   "source": [
    "# Build an autoregressive pipeline lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df3f2f08-76cb-4ec1-af53-9b22ef0dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45432f6f-54b3-430d-8a09-af0e7652782b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Bob and I like'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_text = \"My name is Bob and I like\"\n",
    "input_ids = tokenizer(init_text, return_tensors=\"np\")[\"input_ids\"]\n",
    "tokenizer.decode(input_ids[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc429874-59d9-4abd-90f3-fb2574887e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3666, 1438,  318, 5811,  290,  314,  588]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66e06935-7b12-488d-a5b4-4f002356b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: currently ignoring temperature and top_p\n"
     ]
    }
   ],
   "source": [
    "input_feed = dict(\n",
    "    input_ids = input_ids,\n",
    "    attention_mask = np.ones((1, input_ids.shape[-1]), dtype=int),\n",
    ")\n",
    "output_logits = session.run(output_names=['logits'], input_feed=input_feed,)\n",
    "last_token_logits = output_logits[0][0, -1, :]\n",
    "sampled_token_id = sample_token_id_from_logits(last_token_logits)\n",
    "input_ids = np.append(input_ids, [[sampled_token_id]])  # flattens input_ids implicitly\n",
    "input_ids = einops.rearrange(input_ids, \"i -> 1 i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "42400165-fb8f-4dd7-a5ff-9b587c3ed186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Bob and I like to'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f834935-9f70-4ad0-9350-5c18b19fde93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[3,4]]).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab5693fc-9b78-4747-a8ac-225e97e9b00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3666, 1438,  318, 5811,  290,  314,  588,  284,    1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(input_ids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1350771c-3ec6-472d-8bf7-31429d7778bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.8670678,  -2.8882113,  -6.5249047, ..., -10.038966 ,\n",
       "       -10.068566 ,  -4.1525965], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c674190-eb69-4ffc-aaa3-0cb55540fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: currently ignoring temperature and top_p\n"
     ]
    }
   ],
   "source": [
    "def sample_token_id_from_logits(\n",
    "    logits: np.ndarray, temperature: float = 1.0, top_p: float = 1.0\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Given an array of logits, sample the ID, including temperature \n",
    "    and top_p features.\n",
    "    \"\"\"\n",
    "    print(\"warning: currently ignoring temperature and top_p\")\n",
    "    return np.argmax(logits)\n",
    "\n",
    "assert sample_token_id_from_logits(np.array([-29.858946 , -29.195038 , -30.607428])) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e2d5d609-ea31-4fa1-95b4-fac3ea23abf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29911840-f2ec-4e1d-aa88-0b2a151bdf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a391e-6c74-4886-980a-4304d1fa661a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6aa6e5f-8e97-44b1-aefb-d60c7d2ead01",
   "metadata": {},
   "source": [
    "# check if pipline already supports onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da0029-acce-4cb5-82b0-93841a60cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4e2ac-2f20-4879-b2b3-bf470bdf0f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866db1e-fdbb-430f-adb3-cba06a1ad784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636b23f1-235a-46d1-9459-19182bce0600",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbeb5462-20b3-46e8-84b7-39a2b03d52ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[3666, 1438,  318, 5811,  290,  314,  588]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(init_text, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6450bfd-fc64-4e63-b183-d7ff05f362b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa02eb34-2e7b-444f-8be2-5c16cf3e7f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Entry Not Found for url: https://huggingface.co/lysandre/onnx-bart/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "lysandre/onnx-bart does not appear to have a file named config.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:585\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/file_utils.py:1846\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m-> 1846\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/file_utils.py:2050\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2049\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[0;32m-> 2050\u001b[0m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/file_utils.py:1973\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntryNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m404 Client Error: Entry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevisionNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error: Entry Not Found for url: https://huggingface.co/lysandre/onnx-bart/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bart \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlysandre/onnx-bart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:424\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 424\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:612\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    611\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 612\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:537\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_files\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m~/repos/text-generation/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:613\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    612\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(err)\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    617\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(err)\n",
      "\u001b[0;31mOSError\u001b[0m: lysandre/onnx-bart does not appear to have a file named config.json."
     ]
    }
   ],
   "source": [
    "bart = AutoModelForMaskedLM.from_pretrained(\"lysandre/onnx-bart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c644d335-19ab-41f9-a7d2-5bef143d49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TopPLogitsWarper, TemperatureLogitsWarper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b6ac0aa-cd07-4b48-aeff-36eed2e30d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TemperatureLogitsWarper(0.5)\n",
    "p = TopPLogitsWarper(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1097b71a-b576-4ee8-aa62-1124306f4eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-42., -44., -46.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(input_ids[:,:3], np.array([-21,-22,-23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62825fb4-8f84-4787-ab83-fd6e4fb6cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([-1.5, -2.0, -1.0, -1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0cefd1b9-0117-46d9-b0ac-fbeb4ba39959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2. , -1.5, -1.2, -1. ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(logits,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "20be402c-6019-42d3-8c51-9f7c5eeb2d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 0, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(logits,)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f85d0651-c49d-46e0-8a4a-b8eb636a863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1. , -1.2, -1.5, -2. ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[np.argsort(logits,)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a07c2a34-fccd-4556-8bff-2eee2489cbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -2. , -1. , -1.2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f2098c67-8948-455e-af51-cdba542973e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b3cae1f1-ee4b-4e51-88de-283a421e886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d66a2b63-61c0-4c01-9b3a-55712f3b3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('.')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "53672c00-97e1-468c-8e61-91f05f3525a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/dennis/repos/text-generation/model_creation/hello/hi')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\".\").absolute().joinpath(\"hello/hi\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35de4e2-b288-44ca-8c02-460662ea5930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
