{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a48d82-e1f2-48df-b2c9-c8e7cf644fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import Callable, List, Tuple, Iterable, Dict, Type, Any\n",
    "from functools import reduce, lru_cache\n",
    "from collections import OrderedDict\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence\n",
    "# from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "# import optuna\n",
    "# from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "# from optuna.importance import get_param_importances\n",
    "\n",
    "import wandb\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    # GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6e8ba2bc-f6db-4ef4-91a9-73ab25d7205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCausalLMModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hf_model_name: str, \n",
    "        total_steps: int,\n",
    "        lr: float = 5e-5, \n",
    "        weight_decay: float = 0.01,\n",
    "        adam_epsilon: float = 1e-6,\n",
    "        warmup_steps: int = 1000,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # choose this if want blank slate\n",
    "        # self.config = AutoConfig.from_pretrained(\n",
    "        #     \"gpt2\",\n",
    "        #     vocab_size=len(tokenizer),\n",
    "        #     n_ctx=context_length,\n",
    "        #     bos_token_id=tokenizer.bos_token_id,\n",
    "        #     eos_token_id=tokenizer.eos_token_id,\n",
    "        # )\n",
    "        # self.hf_model = GPT2LMHeadModel(self.config)\n",
    "        # self.hf_model(**self.hf_model.dummy_inputs)  # Builds the model\n",
    "\n",
    "        # choose this if want pre-trained weights\n",
    "        self.hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.wandb_table = wandb.Table(columns=[\"step\", \"text\"])\n",
    "        # self.logger.log_table({\"generated_text\": self.wandb_table})\n",
    "    \n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.hf_model(**inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # visualize the output\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\", model=self.hf_model, tokenizer=tokenizer, device=0\n",
    "        )\n",
    "        txt = \"We develop a method to\"\n",
    "        gen_text = pipe(txt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        # self.wandb_table.add_data(self.global_step, gen_text)\n",
    "        # wandb.log({\"generated_text\": self.wandb_table})\n",
    "        # self.logger.log_table({\"generated_text\": self.wandb_table})\n",
    "        print(gen_text)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model = self.hf_model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.hf_model.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.hf_model.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr=self.hparams.lr, \n",
    "            eps=self.hparams.adam_epsilon\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.hparams.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f105ac6c-bcc2-401e-99f7-859cbb897839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitCausalLMModel(\"distilgpt2\", total_steps=1, lr=1e-4)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "76502e12-15ca-4e9f-b052-2372f1567308",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load(\"models/model.ckpt\", map_location=th.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e291730e-7643-4ee5-a6fa-b6709e096280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6ef18-af41-4c9c-94d9-29d21bc6d02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f1d8b53f-fd18-4627-9b6f-3ebd7deeceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7e5b255a-1b16-4fe9-9c95-58b26c9889b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9396ed04-acbd-4da3-99c3-6e01204296c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pt_model.hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "484e6d9e-9611-41f5-9633-0befb31f729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "274d688f-9e67-4e92-b340-4869155f1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cf09bbb8-50d5-40d5-8d2f-dc646fbe2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6745e055-270f-4ff6-b73e-c8c9aea4aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03038d98-bcd9-4bba-9d76-dca2493260ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3d283-e560-43cc-bcdd-b50b7eb4546d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1468f-c294-4402-8121-d417e5a370cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ab7c4a76-0dc1-4cc9-8021-24ecbca07f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Increasingly many large, more complex systems, such as robots and computers, fail to perform well. In this paper, we propose three algorithms to enhance the performance of reinforcement learning (RL) over standard RL algorithms. First, a neural network's output model, i.e., a generalized maximum likelihood (MLP) algorithm with explicit parameterization or training data, can be learned using only one or more MLP and also the parameters in these learning algorithms. Second, the learning algorithm can\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = (\n",
    "    \"Increasingly many\"\n",
    ")\n",
    "gen_text = pipe(\n",
    "    txt, \n",
    "    num_return_sequences=1, \n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    ")[0][\"generated_text\"]\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "abf0ab2a-eb02-436e-95db-66d6b1534f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15562, 2313, 306, 867, 318, 991, 257, 2408, 4876, 13, 2312, 5050, 11, 2158, 11, 4031, 284, 2987, 262, 2854, 286, 262, 2656, 2746, 393, 2746, 2346, 13, 554, 428, 3348, 11, 356, 9161, 2842, 284, 2987, 262, 9922, 286, 4981, 287, 262, 4732, 286, 7386, 16711, 13, 775, 18077, 734, 5050, 284, 2987, 262, 2854, 286, 262, 2656, 2746, 618, 691, 257, 1178, 10007, 1656, 287, 262, 1459, 2746, 11, 329, 1672, 11, 1262, 2705, 19232, 11, 393, 257, 1178, 7104, 2940, 709, 4981, 13, 554, 6273, 284, 777, 5050, 11, 356, 1205, 284, 9494, 257, 2746, 338, 9922], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'length': [100]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(gen_text, return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e06446-c839-49a5-8f44-dd9a88284fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6eb7db-85d5-45c8-be8c-4d78017b718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.task_specific_params[\"text-generation\"][\"max_length\"] = 100\n",
    "pipe = pipeline(\"text-generation\", model=model.hf_model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9841e557-fe1a-4afd-a236-2c1c87093427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function: take last k tokens (or less) from user input, \n",
    "# apply model, then output last N-k tokens\n",
    "\n",
    "# init_text = \"Using a CNN, we propose a method to\"\n",
    "# max_input_tokens = 3\n",
    "# tokenizer\n",
    "# pipeline\n",
    "# temperature = 1.0\n",
    "# top_p = 1.0\n",
    "\n",
    "def generate_new_text(\n",
    "    init_text: str, \n",
    "    pipeline: Callable, \n",
    "    max_last_input_tokens: int, \n",
    "    temperature=1.0, \n",
    "    top_p=1.0,\n",
    "):\n",
    "    tokenizer = pipeline.tokenizer\n",
    "\n",
    "    init_text_tk = tokenizer(init_text, return_length=True)\n",
    "    num_input_tk = min(init_text_tk[\"length\"][0], max_last_input_tokens)\n",
    "\n",
    "    input_text = tokenizer.decode(init_text_tk[\"input_ids\"][-num_input_tk:])\n",
    "    pipeline_output = pipe(input_text, temperature=temperature, top_p=top_p)\n",
    "    output_text = pipeline_output[0][\"generated_text\"]\n",
    "    \n",
    "    output_text_tk = tokenizer(output_text, return_length=True)\n",
    "    new_text = tokenizer.decode(output_text_tk[\"input_ids\"][num_input_tk:])\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "130b0853-d9b1-4ee9-b4fb-3d43d242965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' discover a robust model of an unsupervised dataset, named the CIFAR10. The method consists of a convolutional neural network (CNN) to train an unsupervised model of an unknown dataset. A CNN is trained in the absence of external labeled examples to avoid the overfitting issue. We report results on the CIFAR100 dataset and the Eureka Challenge datasets, that demonstrates that this is not even possible due to the lack of unlabeled'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_new_text(\n",
    "    \"Using a CNN, we propose a method to newly\",\n",
    "    pipe,\n",
    "    max_last_input_tokens=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98616f4c-cb7e-4fe2-9fde-92bd415dfe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226704a-1c92-4578-b8a9-559e7d7b0808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1dfe502f-20c6-40e7-9c64-f4dd3943b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' CNN can represent the same image, its input, the reconstruction, and the final classification function. The CNN is trained to capture both the information of the source image, the original part, and the reconstruction functions, and then used as input, the reconstruction function, and the reconstruction function. We show that by training CNN on MNIST data with the autoencoder, the CNN'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.task_specific_params[\"text-generation\"][\"max_length\"] = 100\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generate_new_text(\n",
    "    \"Using a CNN, we propose a method to newly characterize MNIST data. By using an autoencoder, the\",\n",
    "    pipe,\n",
    "    max_last_input_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9429d0-aa62-4bac-a2e3-03ad23ae4a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29420a69-053e-4c75-9a1a-11a5b2fb6266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82d9e0-f60c-45a3-bcb1-9e2ca34be556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b16b3b-5e22-4fad-87ad-dbaf486eddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabd502-bb2c-4bca-beeb-9aff437afbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4100f79-2364-49f3-bf28-544c9cf0a396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d197bd8-8add-4488-9e54-a4a48a7f3065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e22e0-9624-4532-a0f7-2f4d635e3c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34debdf8-0cb7-4ee3-bac9-dae5be72af0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b34e0-44c5-4f49-a79f-d03dafd8d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", model=self.hf_model, tokenizer=tokenizer, device=0\n",
    ")\n",
    "txt = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# add two numbers\n",
    "\"\"\"\n",
    "gen_text = pipe(txt, num_return_sequences=1)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
